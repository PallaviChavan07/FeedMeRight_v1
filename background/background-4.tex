% Background chapter continued..

\section{Evaluation Metrics for Recommendation Systems}

In this section we will see few of these methods that are used as evaluation metrics for recommender systems algorithms. Evaluation can be done by two ways, offline and online evaluation \cite{22,23}. In offline analysis collected data is divided into train set and test set in proportion of 80 to 20. The model of recommender system is trained on train dataset and test dataset is hidden from engine. Afterwords build algorithm trained on train dataset is used to predict ratings of unseen items. To understand the quality of recommendation engine, one or combination of evaluation metrics are used. There are several methods available to evaluate the performance of the recommender systems \cite{22,24}. We will see discuss those methods as follow. 
  
\subsection{Recall and Precision}

Recall and precision are most commonly used metrics to evaluate recommendation engines \cite{25}. These metrics can be explained by a confusion matrix \cite{21} as shown in \autoref{fig:confusion_matrix}.
  
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{confusion_matrix}
	\caption{Confusion matrix for calculating recall and precision \cite{21}.
	\label{confusion_matrix} }
	\label{fig:confusion_matrix}
\end{figure}

\noindent Where\\
TP denotes True Positive represents all relevant items are recommended by the system.\\
TN denotes True Negative represents all irrelevant items are correctly not recommended by the system.  \\
FP denotes False Positive represents all irrelevant items which are
incorrectly recommended by the system\\
FN denotes False Negative represents relevant items but system failed to recommend.\\
Based on \autoref{confusion_matrix}, precision is calculated as ratio of the relevant items from recommended items to the number of all recommended items. It is given in \autoref{eq:precision}. 

\begin{equation}
Precision = \frac{TP}{TP + FP} 
\label{eq:precision}
\end{equation}

Based on \autoref{confusion_matrix}, recall is calculated as the ratio of relevant items from recommended items to the number of all relevant items. It is given in \autoref{eq:recall}. 

\begin{equation}
Recall = \frac{TP}{TP + FN} 
\label{eq:recall}
\end{equation}

Larger value of recall and precision implies better recommendations.



\subsection{Mean Absolute Error (MAE)}
Mean absolute error used to calculate the average deviation or error generated from predicted ratings and actual ratings \cite{26}.

\begin{equation}
MAE = \frac{1}{n} \sum_{i=1}^{n}{\vert{Predicted_i - Actual_i} \vert}
\label{eq:mae}
\end{equation}
Where, \\ 
$Predicted_i$ \textsf{ denotes predicted ratings given by user to the item} $i.$ \\
$Actual_i$ \textsf{ denotes actual ratings given by user to the item } $i.$ \\
$n$ \textsf{denotes number of items.}

\noindent With this formula, MAE can calculate general performance of recommender systems but to compare engines with different rating scale, we can normalize MAE by dividing by the mean MAE value as shown in \autoref{eq:nmae}. 

\begin{equation}
NMAE = \frac{MAE} {Rating_{max} - Rating_{min}}
\label{eq:nmae}
\end{equation}


\subsection{Root Mean Square Error (RMSE)}
RMSE is a variation of MAE. It also measures the average magnitude of the error. But it puts more weight on large errors as shown in \autoref{eq:rmse}. RMSE can be defines as square root of the average of squared deviations between predicted ratings and real ratings.

\begin{equation}
RMSE = \sqrt{ \frac{1}{n} \sum_{i=1}^{n}{({Predicted_i - Actual_i} ) ^ {2}}}
\label{eq:rmse}
\end{equation}
Where, \\
$Predicted_i$ \textsf{ denotes predicted ratings given by user to the item} $i.$ \\
$Actual_i$ \textsf{ denotes actual ratings given by user to the item } $i.$ \\
$n$ \textsf{denotes number of items.}

\noindent Normalized RMSE is given in \autoref{eq:nrmse}
\begin{equation}
NRMSE = \frac{RMSE} {Rating_{max} - Rating_{min}}
\label{eq:nrmse}
\end{equation}



\subsection{Mean Reciprocal Rank}

Mean Reciprocal Rank is one of the rank-aware evaluation metrics.
It is retrieval measure which calculates the reciprocal of the rank at which the first relevant document was recommended \cite{27}.
From the list of generated recommendations, it finds the rank of first relevant recommended item and computes the reciprocal of that rank. It can be calucluted as shown in \autoref{eq:mrr}

\begin{equation}
MRR(O, U) = \frac{1}{\vert U \vert} \sum_{u \in \epsilon} \frac{1} {k_{u}}
\label{eq:mrr}
\end{equation}
\noindent Where, \\
$k_{u}$ denotes first relevant recommended item for user u. \\
$U$  denotes number of users. \\
MRR focuses on the first relevant item of the recommended list.HIgher reciprocal rank implies better recommendation engine.


\subsection{Mean Average Precision at cutoff k (MAP at k)}
MAP at k considers the subset of list recommended by the system while measuring precision. It does not concern about ordering the items in the list. For each user for each relevant item it computes the precision of list through that item. After that it average sub list precisions. Mathematically it is show in \autoref{eq:map_at_k}

\begin{equation}
MAP_{i} = \frac{1}{\vert R_{i} \vert} \sum_{k=1}^{\vert R_{i} \vert} { P( R_{i}[k]) }
\label{eq:map_at_k}
\end{equation}

\noindent Where, \\
$R_{i}$ denotes rating for item $i$. \\
$k$ denotes rank from 1 to k.\\
$P$ denotes precision of first relevant item.

\noindent Higher mean precision value implies correct recommendations.
%\subsection{NDCG (Normalized Discounted Cumulative Gain)}
%--------- To DO ---------